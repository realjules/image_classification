{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":83922,"databundleVersionId":9626915,"sourceType":"competition"}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# HW2P2: Image Recognition and Verification","metadata":{"id":"UbXkUQWFLBRF"}},{"cell_type":"markdown","source":"This is the second homework  in 11785: Introduction to Deep Learning. We are trying to tackle the problem of Image Verification. For this, we will need to first train our own CNN model to tackle the problem of classification, consisting of 8631 identities. Using this, we get the face embeddings for different pairs of images and try to identify if the pair of face matches or not.","metadata":{"id":"VMg74_LaLL55"}},{"cell_type":"markdown","source":"","metadata":{"id":"oaUgdhRqy8oT"}},{"cell_type":"markdown","source":"# Libraries","metadata":{"id":"695K5zs36a48"}},{"cell_type":"code","source":"!pip install wandb --quiet # Install WandB\n!pip install pytorch_metric_learning --quiet #Install the Pytorch Metric Library\n!pip install torchsummary==1.1.0 wandb --quiet","metadata":{"id":"MmbTatic6PDX","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom torchsummary import summary\nimport torchvision\nfrom torchvision import transforms\nimport torch.nn.functional as F\nimport os\nimport gc\nfrom tqdm import tqdm\nfrom PIL import Image\nimport numpy as np\nimport pandas as pd\nfrom sklearn.metrics import accuracy_score\nfrom sklearn import metrics as mt\nfrom scipy.optimize import brentq\nfrom scipy.interpolate import interp1d\nimport glob\nimport wandb\nimport matplotlib.pyplot as plt\nfrom pytorch_metric_learning import samplers\nimport csv\n\nDEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint(\"Device: \", DEVICE)","metadata":{"id":"_oCzGBTh6xjL","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Kaggle","metadata":{"id":"gf6Da1K37BSJ"}},{"cell_type":"code","source":"!pip install --upgrade --force-reinstall --no-deps kaggle==1.5.8\n!mkdir /root/.kaggle\n\nwith open(\"/root/.kaggle/kaggle.json\", \"w+\") as f:\n    f.write('{\"username\":\"julesudahemuka\",\"key\":\"17cc4bc4889c35fd17d92f92e0eede77\"}')\n    # Put your kaggle username & key here\n\n!chmod 600 /root/.kaggle/kaggle.json","metadata":{"id":"Z1Uu5z2K7AS3","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Reminder: Make sure you have connected your kaggle API before running this block\n!mkdir '/content/data'\n\n!kaggle competitions download -c 11785-hw-2-p-2-face-verification-fall-2024\n!unzip -qo '11785-hw-2-p-2-face-verification-fall-2024.zip' -d '/content/data'","metadata":{"id":"sNRYbTmU7Dk3","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Config","metadata":{"id":"9OgkfYwP7HVt"}},{"cell_type":"code","source":"config = {\n    'batch_size': 32, # Increase this if your GPU can handle it\n    'lr': 0.0058,\n    'epochs': 30, # 20 epochs is recommended ONLY for the early submission - you will have to train for much longer typically.\n    'data_dir': \"/kaggle/input/11785-hw-2-p-2-face-verification-fall-2024/11-785-f24-hw2p2-verification/cls_data/\",\n    'data_ver_dir': \"/kaggle/input/11785-hw-2-p-2-face-verification-fall-2024/11-785-f24-hw2p2-verification/ver_data/\",\n    'checkpoint_dir': \"/kaggle/working/\",\n    'weight_decay': 1e-4,  # L2 regularization\n    'momentum': 0.9,  # If using SGD optimizer\n    'num_workers': 8,  # For data loading, adjust based on your CPU\n    'pin_memory': True,  # Can speed up data transfer to GPU\n    'scheduler_step_size': 5,  # If using StepLR scheduler\n    'scheduler_gamma': 0.1,  # Factor to reduce LR by\n    'num_classes': 8631,  # As mentioned in the problem description\n    'weight_decay': 1e-2\n}","metadata":{"id":"CMXkHmFc7G9m","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Dataset","metadata":{"id":"EEAW65sB8Wlp"}},{"cell_type":"markdown","source":"## Dataset Class for doing Image Verification","metadata":{"id":"mPSk8DyK8htk"}},{"cell_type":"code","source":"class ImagePairDataset(torch.utils.data.Dataset):\n    def __init__(self, data_dir, csv_file, transform):\n        self.data_dir = data_dir\n        self.transform = transform\n        self.pairs = []\n        if csv_file.endswith('.csv'):\n            with open(csv_file, 'r') as f:\n                reader = csv.reader(f)\n                for i, row in enumerate(reader):\n                    if i == 0:\n                        continue\n                    else:\n                        self.pairs.append(row)\n        else:\n            with open(csv_file, 'r') as f:\n                for line in f.readlines():\n                    self.pairs.append(line.strip().split(' '))\n\n    def __len__(self):\n        return len(self.pairs)\n\n    def __getitem__(self, idx):\n        img_path1, img_path2, match = self.pairs[idx]\n        img1 = Image.open(os.path.join(self.data_dir, img_path1)).convert('RGB')\n        img2 = Image.open(os.path.join(self.data_dir, img_path2)).convert('RGB')\n        \n        # Convert PIL Images to numpy arrays\n        img1 = np.array(img1)\n        img2 = np.array(img2)\n\n        if self.transform:\n            transformed1 = self.transform(image=img1)\n            transformed2 = self.transform(image=img2)\n            img1 = transformed1['image']\n            img2 = transformed2['image']\n\n        return img1, img2, int(match)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class TestImagePairDataset(torch.utils.data.Dataset):\n\n    def __init__(self, data_dir, csv_file, transform):\n        self.data_dir = data_dir\n        self.transform = transform\n        self.pairs = []\n        if csv_file.endswith('.csv'):\n            with open(csv_file, 'r') as f:\n                reader = csv.reader(f)\n                for i, row in enumerate(reader):\n                    if i == 0:\n                        continue\n                    else:\n                        self.pairs.append(row)\n        else:\n            with open(csv_file, 'r') as f:\n                for line in f.readlines():\n                    self.pairs.append(line.strip().split(' '))\n\n    def __len__(self):\n        return len(self.pairs)\n\n    def __getitem__(self, idx):\n        img_path1, img_path2 = self.pairs[idx]\n        img1 = Image.open(os.path.join(self.data_dir, img_path1)).convert('RGB')\n        img2 = Image.open(os.path.join(self.data_dir, img_path2)).convert('RGB')\n\n        img1 = np.array(img1)\n        img2 = np.array(img2)\n\n        if self.transform:\n            transformed1 = self.transform(image=img1)\n            transformed2 = self.transform(image=img2)\n            img1 = transformed1['image']\n            img2 = transformed2['image']\n\n        return img1, img2","metadata":{"id":"xgBHYshwN9VY","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Create Dataloaders for Image Recognition","metadata":{"id":"2j24TXNo9P97"}},{"cell_type":"code","source":"import albumentations as A\nfrom albumentations.pytorch import ToTensorV2\n\ndata_dir = config['data_dir']\ntrain_dir = os.path.join(data_dir, 'train')\nval_dir = os.path.join(data_dir, 'dev')\n\n# Advanced train transforms\ntrain_transforms = A.Compose([\n    A.RandomResizedCrop(height=112, width=112, scale=(0.8, 1.0)),\n    A.HorizontalFlip(p=0.5),\n    A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.5),\n    A.HueSaturationValue(hue_shift_limit=10, sat_shift_limit=20, val_shift_limit=10, p=0.5),\n    A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.1, rotate_limit=15, p=0.5),\n    A.CoarseDropout(max_holes=8, max_height=8, max_width=8, fill_value=0, p=0.2),\n    A.GaussNoise(var_limit=(10.0, 50.0), p=0.2),\n    A.RandomGamma(gamma_limit=(80, 120), p=0.2),\n    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n    ToTensorV2(),\n])\n\n# Val transforms\nval_transforms = A.Compose([\n    A.Resize(height=128, width=128),\n    A.CenterCrop(height=112, width=112),\n    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n    ToTensorV2(),\n])\n\nclass AlbumentationsDataset(torch.utils.data.Dataset):\n    def __init__(self, root, transform=None):\n        self.dataset = torchvision.datasets.ImageFolder(root)\n        self.transform = transform\n        self.classes = self.dataset.classes\n        self.class_to_idx = self.dataset.class_to_idx\n\n    def __getitem__(self, idx):\n        img, label = self.dataset[idx]\n        if self.transform:\n            augmented = self.transform(image=np.array(img))\n            img = augmented['image']\n        return img, label\n\n    def __len__(self):\n        return len(self.dataset)\n\n\n# Get datasets\ntrain_dataset = AlbumentationsDataset(train_dir, transform=train_transforms)\nval_dataset = AlbumentationsDataset(val_dir, transform=val_transforms)\n\n# Data loaders\ntrain_loader = torch.utils.data.DataLoader(\n    train_dataset,\n    batch_size=config[\"batch_size\"],\n    shuffle=True,\n    pin_memory=True,\n    num_workers=8,\n    persistent_workers=True\n)\n\nval_loader = torch.utils.data.DataLoader(\n    val_dataset,\n    batch_size=config[\"batch_size\"],\n    shuffle=False,\n    num_workers=4,\n    persistent_workers=True\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import csv\nimport os\n\ndef convert_txt_to_csv_val(input_file, output_file):\n    with open(input_file, 'r') as f_in, open(output_file, 'w', newline='') as f_out:\n        csv_writer = csv.writer(f_out)\n        csv_writer.writerow(['image1', 'image2', 'label'])  # Header\n        \n        for line in f_in:\n            parts = line.strip().split()\n            if len(parts) == 3:\n                csv_writer.writerow(parts)\n            else:\n                print(f\"Skipping invalid line: {line.strip()}\")\n\n\ndef convert_txt_to_csv_test(input_file, output_file):\n    with open(input_file, 'r') as f_in, open(output_file, 'w', newline='') as f_out:\n        csv_writer = csv.writer(f_out)\n        csv_writer.writerow(['image1', 'image2'])  # Header\n        \n        for line in f_in:\n            parts = line.strip().split()\n            if len(parts) == 2:\n                csv_writer.writerow(parts)\n            else:\n                print(f\"Skipping invalid line: {line.strip()}\")\n\n# Input files\ntest_input = '/kaggle/input/11785-hw-2-p-2-face-verification-fall-2024/11-785-f24-hw2p2-verification/test_pairs.txt'\nval_input = '/kaggle/input/11785-hw-2-p-2-face-verification-fall-2024/11-785-f24-hw2p2-verification/val_pairs.txt'\n\n# Output files in the current working directory\ntest_output = 'test_pairs.csv'\nval_output = 'val_pairs.csv'\n\n# Convert test pairs\nconvert_txt_to_csv_test(test_input, test_output)\n\n# Convert validation pairs\nconvert_txt_to_csv_val(val_input, val_output)\n\nprint(\"Conversion completed. CSV files have been created in the current working directory.\")\n\n# Print the contents of the current directory to verify\nprint(\"\\nContents of the current directory:\")\nprint(os.listdir('.'))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data_dir = config['data_ver_dir']\n\n# For validation dataset\npair_dataset = ImagePairDataset(data_dir, csv_file='val_pairs.csv', transform=val_transforms)\npair_dataloader = torch.utils.data.DataLoader(pair_dataset,\n                                              batch_size=config[\"batch_size\"],\n                                              shuffle=False,\n                                              pin_memory=True,\n                                              num_workers=4)\n\n# For test dataset\ntest_pair_dataset = TestImagePairDataset(data_dir, csv_file='test_pairs.csv', transform=val_transforms)\ntest_pair_dataloader = torch.utils.data.DataLoader(test_pair_dataset,\n                                              batch_size=config[\"batch_size\"],\n                                              shuffle=False,\n                                              pin_memory=True,\n                                              num_workers=4)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# EDA and Viz","metadata":{"id":"436KzM6u-3A2"}},{"cell_type":"code","source":"# Rest of the code remains the same\n\n# Double-check your dataset/dataloaders work as expected\nprint(\"Number of classes    : \", len(train_dataset.classes))\nprint(\"No. of train images  : \", len(train_dataset))\nprint(\"Shape of image       : \", train_dataset[0][0].shape)\nprint(\"Batch size           : \", config['batch_size'])\nprint(\"Train batches        : \", len(train_loader))\nprint(\"Val batches          : \", len(val_loader))\n\n# Additional checks\n# print(\"Class to index mapping:\", train_dataset.class_to_idx)\nprint(\"Sample label:\", train_dataset[0][1])\nprint(\"Corresponding class:\", train_dataset.classes[train_dataset[0][1]])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\nfrom torchvision.utils import make_grid\n\n# Visualize a few images in the dataset\ndef show_transformed_images(dataset, num_images=25):\n    r, c = 5, 5\n    fig, axes = plt.subplots(r, c, figsize=(15, 15))\n    \n    for i in range(num_images):\n        img, label = dataset[i]\n        img = img.permute(1, 2, 0).numpy()\n        img = (img * 255).astype(np.uint8)  # Denormalize\n        \n        row = i // c\n        col = i % c\n        axes[row, col].imshow(img)\n        axes[row, col].axis('off')\n        axes[row, col].set_title(f\"Class: {dataset.classes[label]}\")\n    \n    plt.tight_layout()\n    plt.show()\n\n# Show transformed training images\nshow_transformed_images(train_dataset)\n\n# If you want to visualize the original images without augmentation\noriginal_dataset = torchvision.datasets.ImageFolder(train_dir, transform=transforms.ToTensor())\nshow_transformed_images(original_dataset)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Visualize specific augmentations\nspecific_transforms = A.Compose([\n    A.RandomResizedCrop(height=112, width=112, scale=(0.8, 1.0)),\n    A.HorizontalFlip(p=1.0),  # Always apply horizontal flip for visualization\n    A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=1.0),\n    ToTensorV2(),\n])\n\nspecific_dataset = AlbumentationsDataset(train_dir, transform=specific_transforms)\nshow_transformed_images(specific_dataset)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Model Architecture","metadata":{"id":"y3TUocDw_JU_"}},{"cell_type":"markdown","source":"FAQ:\n\n**What's a very low early deadline architecture (mandatory early submission)**?\n\n- The very low early deadline architecture is a 5-layer CNN. Keep in mind the parameter limit is 18M.\n- The first convolutional layer has 64 channels, kernel size 7, and stride 4. The next three have 128, 256, 512 and 1024 channels. Each have kernel size 3 and stride 2. Documentation to make convolutional layers: https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html\n- Think about strided convolutions from the lecture, as convolutions with stride = 1 and downsampling. For strided convolution, what padding do you need for preserving the spatial resolution? (Hint => padding = kernel_size // 2) - Think why?\n- Each convolutional layer is accompanied by a Batchnorm and ReLU layer.\n- Finally, you want to average pool over the spatial dimensions to reduce them to 1 x 1. Use AdaptiveAvgPool2d. Documentation for AdaptiveAvgPool2d: https://pytorch.org/docs/stable/generated/torch.nn.AdaptiveAvgPool2d.html\n- Then, remove (Flatten?) these trivial 1x1 dimensions away.\nLook through https://pytorch.org/docs/stable/nn.html\n\n\n**Why does a very simple network have 4 convolutions**?\n\nInput images are 112x112. Note that each of these convolutions downsample. Downsampling 2x effectively doubles the receptive field, increasing the spatial region each pixel extracts features from. Downsampling 32x is standard for most image models.\n\n**Why does a very simple network have high channel sizes**?\n\nEvery time you downsample 2x, you do 4x less computation (at same channel size). To maintain the same level of computation, you 2x increase # of channels, which increases computation by 4x. So, balances out to same computation. Another intuition is - as you downsample, you lose spatial information. We want to preserve some of it in the channel dimension.\n\n**What is return_feats?**\n\nIt essentially returns the second-to-last-layer features of a given image. It's a \"feature encoding\" of the input image, and you can use it for the verification task. You would use the outputs of the final classification layer for the classification task. You might also find that the classification outputs are sometimes better for verification too - try both.","metadata":{"id":"yIR-dTHYyUov"}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.checkpoint import checkpoint\n\n\nclass SEBlock(nn.Module):\n    def __init__(self, channel, reduction=16):\n        super(SEBlock, self).__init__()\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.fc = nn.Sequential(\n            nn.Linear(channel, channel // reduction, bias=False),\n            nn.ReLU(inplace=True),\n            nn.Linear(channel // reduction, channel, bias=False),\n            nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        b, c, _, _ = x.size()\n        y = self.avg_pool(x).view(b, c)\n        y = self.fc(y).view(b, c, 1, 1)\n        return x * y.expand_as(x)\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None, use_se=True):\n        super(Bottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.conv3 = nn.Conv2d(planes, planes * self.expansion, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(planes * self.expansion)\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n        self.use_se = use_se\n        if use_se:\n            self.se = SEBlock(planes * self.expansion)\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.use_se:\n            out = self.se(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\nclass CustomResNet50(nn.Module):\n    def __init__(self, num_classes=8631):\n        super(CustomResNet50, self).__init__()\n        self.inplanes = 64\n        self.use_checkpointing = True\n        \n        # Adjusted initial convolutions for smaller input sizes\n        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(32)\n        self.conv2 = nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(32)\n        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(64)\n        self.relu = nn.ReLU(inplace=True)\n        \n        self.layer1 = self._make_layer(Bottleneck, 64, 3)\n        self.layer2 = self._make_layer(Bottleneck, 128, 4, stride=2)\n        self.layer3 = self._make_layer(Bottleneck, 256, 6, stride=2)\n        self.layer4 = self._make_layer(Bottleneck, 512, 3, stride=2)\n        \n        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n        self.dropout = nn.Dropout(0.5)\n        self.fc = nn.Linear(512 * Bottleneck.expansion, num_classes)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n\n    def _make_layer(self, block, planes, blocks, stride=1):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.inplanes, planes * block.expansion,\n                          kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(planes * block.expansion),\n            )\n\n        layers = []\n        layers.append(block(self.inplanes, planes, stride, downsample))\n        self.inplanes = planes * block.expansion\n        for _ in range(1, blocks):\n            layers.append(block(self.inplanes, planes))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.relu(self.bn1(self.conv1(x)))\n        x = self.relu(self.bn2(self.conv2(x)))\n        x = self.relu(self.bn3(self.conv3(x)))\n        \n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n\n        \n        x = self.avgpool(x)\n        feats = torch.flatten(x, 1)\n        feats = self.dropout(feats)\n        out = self.fc(feats)\n\n        return {\"feats\": feats, \"out\": out}\n\n# Initialize the model\nmodel = CustomResNet50(num_classes=8631).to(DEVICE)\n\n# Check the number of parameters\ntotal_params = sum(p.numel() for p in model.parameters())\nprint(f\"Total parameters: {total_params}\")","metadata":{"id":"4LLX2Rki_LzA","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\n\n# Defining Loss function\ncriterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n\n# Defining Optimizer\noptimizer = torch.optim.AdamW(model.parameters(), lr=config['lr'], weight_decay=config['weight_decay'])\n\n# # Defining Scheduler\nscheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=config['epochs'], eta_min=1e-6)\n\n# Initialising mixed-precision training.\nscaler = torch.cuda.amp.GradScaler()","metadata":{"id":"pDP--pND_3Vy","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Metrics","metadata":{"id":"-d5ZDQfpw7gR"}},{"cell_type":"code","source":"class AverageMeter:\n    \"\"\"Computes and stores the average and current value\"\"\"\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count","metadata":{"id":"7Ecg0J2sw9jJ","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def accuracy(output, target, topk=(1,)):\n    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n    maxk = min(max(topk), output.size()[1])\n    batch_size = target.size(0)\n    _, pred = output.topk(maxk, 1, True, True)\n    pred = pred.t()\n    correct = pred.eq(target.reshape(1, -1).expand_as(pred))\n    return [correct[:min(k, maxk)].reshape(-1).float().sum(0) * 100. / batch_size for k in topk]","metadata":{"id":"TqVw0ab0xBKT","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_ver_metrics(labels, scores, FPRs):\n    # eer and auc\n    fpr, tpr, _ = mt.roc_curve(labels, scores, pos_label=1)\n    roc_curve = interp1d(fpr, tpr)\n    EER = 100. * brentq(lambda x : 1. - x - roc_curve(x), 0., 1.)\n    AUC = 100. * mt.auc(fpr, tpr)\n\n    # get acc\n    tnr = 1. - fpr\n    pos_num = labels.count(1)\n    neg_num = labels.count(0)\n    ACC = 100. * max(tpr * pos_num + tnr * neg_num) / len(labels)\n\n    # TPR @ FPR\n    if isinstance(FPRs, list):\n        TPRs = [\n            ('TPR@FPR={}'.format(FPR), 100. * roc_curve(float(FPR)))\n            for FPR in FPRs\n        ]\n    else:\n        TPRs = []\n\n    return {\n        'ACC': ACC,\n        'EER': EER,\n        'AUC': AUC,\n        'TPRs': TPRs,\n    }","metadata":{"id":"uNCQjz2RxD5S","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Train and Validation Function","metadata":{"id":"juUbZnP0AEUi"}},{"cell_type":"code","source":"def train_epoch(model, dataloader, optimizer, lr_scheduler, scaler, device, config):\n\n    model.train()\n\n    # metric meters\n    loss_m = AverageMeter()\n    acc_m = AverageMeter()\n\n    # Progress Bar\n    batch_bar = tqdm(total=len(dataloader), dynamic_ncols=True, leave=False, position=0, desc='Train', ncols=5)\n\n    for i, (images, labels) in enumerate(dataloader):\n\n        optimizer.zero_grad() # Zero gradients\n\n        # send to cuda\n        images = images.to(device, non_blocking=True)\n        if isinstance(labels, (tuple, list)):\n            targets1, targets2, lam = labels\n            labels = (targets1.to(device), targets2.to(device), lam)\n        else:\n            labels = labels.to(device, non_blocking=True)\n\n        # forward\n        with torch.cuda.amp.autocast():  # This implements mixed precision. Thats it!\n            outputs = model(images)\n\n            # Use the type of output depending on the loss function you want to use\n            loss = criterion(outputs['out'], labels)\n\n        scaler.scale(loss).backward() # This is a replacement for loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n        scaler.step(optimizer) # This is a replacement for optimizer.step()\n        scaler.update()\n        # metrics\n        loss_m.update(loss.item())\n        if 'feats' in outputs:\n            acc = accuracy(outputs['out'], labels)[0].item()\n        else:\n            acc = 0.0\n        acc_m.update(acc)\n\n        # tqdm lets you add some details so you can monitor training as you train.\n        batch_bar.set_postfix(\n            # acc         = \"{:.04f}%\".format(100*accuracy),\n            acc=\"{:.04f}% ({:.04f})\".format(acc, acc_m.avg),\n            loss        = \"{:.04f} ({:.04f})\".format(loss.item(), loss_m.avg),\n            lr          = \"{:.04f}\".format(float(optimizer.param_groups[0]['lr'])))\n\n        batch_bar.update() # Update tqdm bar\n\n    # You may want to call some schedulers inside the train function. What are these?\n    if lr_scheduler is not None:\n        lr_scheduler.step()\n\n    batch_bar.close()\n\n    return acc_m.avg, loss_m.avg","metadata":{"id":"IMnxvQT-AHsu","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"@torch.no_grad()\ndef valid_epoch_cls(model, dataloader, device, config):\n\n    model.eval()\n    batch_bar = tqdm(total=len(dataloader), dynamic_ncols=True, position=0, leave=False, desc='Val Cls.', ncols=5)\n\n    # metric meters\n    loss_m = AverageMeter()\n    acc_m = AverageMeter()\n\n    for i, (images, labels) in enumerate(dataloader):\n\n        # Move images to device\n        images, labels = images.to(device), labels.to(device)\n\n        # Get model outputs\n        with torch.inference_mode():\n            outputs = model(images)\n            loss = criterion(outputs['out'], labels)\n\n        # metrics\n        acc = accuracy(outputs['out'], labels)[0].item()\n        loss_m.update(loss.item())\n        acc_m.update(acc)\n\n        batch_bar.set_postfix(\n            acc         = \"{:.04f}% ({:.04f})\".format(acc, acc_m.avg),\n            loss        = \"{:.04f} ({:.04f})\".format(loss.item(), loss_m.avg))\n\n        batch_bar.update()\n\n    batch_bar.close()\n    return acc_m.avg, loss_m.avg","metadata":{"id":"5qkdH295wNUX","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"gc.collect() # These commands help you when you face CUDA OOM error\ntorch.cuda.empty_cache()","metadata":{"id":"-Yan5vLDyj-3","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Verification Task","metadata":{"id":"0q1gRMAsyknz"}},{"cell_type":"code","source":"def valid_epoch_ver(model, pair_data_loader, device, config):\n\n    model.eval()\n    scores = []\n    match_labels = []\n    batch_bar = tqdm(total=len(pair_data_loader), dynamic_ncols=True, position=0, leave=False, desc='Val Veri.')\n    for i, (images1, images2, labels) in enumerate(pair_data_loader):\n\n        # match_labels = match_labels.to(device)\n        images = torch.cat([images1, images2], dim=0).to(device)\n        # Get model outputs\n        with torch.inference_mode():\n            outputs = model(images)\n\n        feats = F.normalize(outputs['feats'], dim=1)\n        feats1, feats2 = feats.chunk(2)\n        similarity = F.cosine_similarity(feats1, feats2)\n        scores.append(similarity.cpu().numpy())\n        match_labels.append(labels.cpu().numpy())\n        batch_bar.update()\n\n    scores = np.concatenate(scores)\n    match_labels = np.concatenate(match_labels)\n\n    FPRs=['1e-4', '5e-4', '1e-3', '5e-3', '5e-2']\n    metric_dict = get_ver_metrics(match_labels.tolist(), scores.tolist(), FPRs)\n    print(metric_dict)\n\n    return metric_dict['ACC']","metadata":{"id":"SSGeDCi-wa1W","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# WandB","metadata":{"id":"piblCbe5yotj"}},{"cell_type":"code","source":"import wandb, os\nwandb.login(key=\"bc949bf92a679253de4baf498494ef4e3fdbbf00\") #API Key is in your wandb account, under settings (wandb.ai/settings)\n\n# Function to load model and optimizer from wandb checkpoint\ndef load_model_from_wandb(run_id, model, device='cuda'):\n    # Load the model checkpoint from wandb\n    api = wandb.Api()\n    run = api.run(f\"judahemu-carnegie-mellon-university/hw2p2-ablations/{run_id}\")\n    weights_file = run.file(\"working/best_cls.pth\")\n    downloaded_path = weights_file.download(replace=True)\n    \n    # Load the checkpoint\n    checkpoint = torch.load(downloaded_path.name, map_location=device)\n    \n    # Load model weights\n    model.load_state_dict(checkpoint['model_state_dict'])\n    \n    return model\n\n# Using of the load_model_from_wandb function:\nmodel = CustomResNet50(num_classes=8631).to(DEVICE)  # Initialize your model architecture\n\n# Load model, optimizer, and scheduler from wandb checkpoint\nmodel = load_model_from_wandb(\"ta8wa23u\", model, device=DEVICE)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create your wandb run\nrun = wandb.init(\n    name = \"early-submission\", ## Wandb creates random run names if you skip this field\n#     reinit = True, ### Allows reinitalizing runs when you re-run this cell\n#     run_id = ### Insert specific run id here if you want to resume a previous run\n    # resume = \"must\" ### You need this to resume previous runs, but comment out reinit = True when using this\n    project = \"hw2p2-ablations\", ### Project should be created in your wandb account\n    config = config ### Wandb Config for your run\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Create your wandb run\n# run = wandb.init(\n#     name = \"early-submission\", ## Wandb creates random run names if you skip this field\n# #     reinit = True, ### Allows reinitalizing runs when you re-run this cell\n# #     run_id = ### Insert specific run id here if you want to resume a previous run\n#     # resume = \"must\" ### You need this to resume previous runs, but comment out reinit = True when using this\n#     project = \"hw2p2-ablations\", ### Project should be created in your wandb account\n#     config = config ### Wandb Config for your run\n# )","metadata":{"id":"GLNNqwV4ysNP","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Checkpointing and Loading Model","metadata":{"id":"t0RrtpFKzH3k"}},{"cell_type":"code","source":"# Uncomment the line for saving the scheduler save dict if you are using a scheduler\ndef save_model(model, optimizer, scheduler, metrics, epoch, path):\n    torch.save(\n        {'model_state_dict'         : model.state_dict(),\n         'optimizer_state_dict'     : optimizer.state_dict(),\n         'scheduler_state_dict'     : scheduler.state_dict(),\n         'metric'                   : metrics,\n         'epoch'                    : epoch},\n         path)\n\n\ndef load_model(model, optimizer=None, scheduler=None, path='./checkpoint.pth'):\n    checkpoint = torch.load(path)\n    model.load_state_dict(checkpoint['model_state_dict'])\n    if optimizer is not None:\n        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n    else:\n        optimizer = None\n    if scheduler is not None:\n        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n    else:\n        scheduler = None\n    epoch = checkpoint['epoch']\n    metrics = checkpoint['metric']\n    return model, optimizer, scheduler, epoch, metrics","metadata":{"id":"dDFmC8hpzLOq","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Experiments","metadata":{"id":"wpFT7iriy5bi"}},{"cell_type":"code","source":"e = 20\nbest_valid_cls_acc = 0.0\neval_cls = True\nbest_valid_ret_acc = 0.0\nfor epoch in range(e, config['epochs']):\n        # epoch\n        print(\"\\nEpoch {}/{}\".format(epoch+1, config['epochs']))\n\n        # train\n        train_cls_acc, train_loss = train_epoch(model, train_loader, optimizer, scheduler, scaler, DEVICE, config)\n        curr_lr = float(optimizer.param_groups[0]['lr'])\n        print(\"\\nEpoch {}/{}: \\nTrain Cls. Acc {:.04f}%\\t Train Cls. Loss {:.04f}\\t Learning Rate {:.04f}\".format(epoch + 1, config['epochs'], train_cls_acc, train_loss, curr_lr))\n        metrics = {\n            'train_cls_acc': train_cls_acc,\n            'train_loss': train_loss,\n        }\n        # classification validation\n        if eval_cls:\n            valid_cls_acc, valid_loss = valid_epoch_cls(model, val_loader, DEVICE, config)\n            print(\"Val Cls. Acc {:.04f}%\\t Val Cls. Loss {:.04f}\".format(valid_cls_acc, valid_loss))\n            metrics.update({\n                'valid_cls_acc': valid_cls_acc,\n                'valid_loss': valid_loss,\n            })\n\n        # retrieval validation\n        valid_ret_acc = valid_epoch_ver(model, pair_dataloader, DEVICE, config)\n        print(\"Val Ret. Acc {:.04f}%\".format(valid_ret_acc))\n        metrics.update({\n            'valid_ret_acc': valid_ret_acc\n        })\n\n        # save model\n        save_model(model, optimizer, scheduler, metrics, epoch, os.path.join(config['checkpoint_dir'], 'last.pth'))\n        print(\"Saved epoch model\")\n\n        # save best model\n        if eval_cls:\n            if valid_cls_acc >= best_valid_cls_acc:\n                best_valid_cls_acc = valid_cls_acc\n                save_model(model, optimizer, scheduler, metrics, epoch, os.path.join(config['checkpoint_dir'], 'best_cls.pth'))\n                wandb.save(os.path.join(config['checkpoint_dir'], 'best_cls.pth'))\n                print(\"Saved best classification model\")\n\n        if valid_ret_acc >= best_valid_ret_acc:\n            best_valid_ret_acc = valid_ret_acc\n            save_model(model, optimizer, scheduler, metrics, epoch, os.path.join(config['checkpoint_dir'], 'best_ret.pth'))\n            wandb.save(os.path.join(config['checkpoint_dir'], 'best_ret.pth'))\n            print(\"Saved best retrieval model\")\n\n        # log to tracker\n        if run is not None:\n            run.log(metrics)","metadata":{"id":"59FcCeJfy3Zm","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Testing and Kaggle Submission (Verification)","metadata":{"id":"7aZ7yRdBKraO"}},{"cell_type":"code","source":"def test_epoch_ver(model, pair_data_loader, config):\n\n    model.eval()\n    scores = []\n    batch_bar = tqdm(total=len(pair_data_loader), dynamic_ncols=True, position=0, leave=False, desc='Val Veri.')\n    for i, (images1, images2) in enumerate(pair_data_loader):\n\n        images = torch.cat([images1, images2], dim=0).to(DEVICE)\n        \n        # Get model outputs\n        with torch.inference_mode():\n            outputs = model(images)\n\n        feats = F.normalize(outputs['feats'], dim=1)\n        feats1, feats2 = feats.chunk(2)\n        similarity = F.cosine_similarity(feats1, feats2)\n        scores.extend(similarity.cpu().numpy().tolist())\n        batch_bar.update()\n\n    return scores","metadata":{"id":"XUAa3m2h0eCD","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"scores = test_epoch_ver(model, test_pair_dataloader, config)","metadata":{"id":"KZob_EOvIb65","outputId":"aec87391-d825-4eb4-f439-7a97681dbd39","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"with open(\"verification_early_submissions.csv\", \"w+\") as f:\n    f.write(\"ID,Label\\n\")\n    for i in range(len(scores)):\n        f.write(\"{},{}\\n\".format(i, scores[i]))","metadata":{"id":"ETSImix3AYy_","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"### Submit to kaggle competition using kaggle API (Uncomment below to use)\n!kaggle competitions submit -c 11785-hw-2-p-2-face-verification-fall-2024 -f verification_early_submission.csv -m \"Message\"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}